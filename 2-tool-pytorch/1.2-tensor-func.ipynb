{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 基本运算示例\n",
    "\n",
    "Torch的tensor和Numpy的Array，Pandas的Series和Dataframe类似，是了解应用这些代码库的基础，这里首先简单快速地了解Tensor及如何使用GPU，然后日常积累一些常用张量运算函数。\n",
    "\n",
    "主要参考：\n",
    "\n",
    "- [TENSORS](https://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html)\n",
    "- [Speed Up your Algorithms Part 1 — PyTorch](https://towardsdatascience.com/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051)\n",
    "- [PyTorch 101, Part 1: Understanding Graphs, Automatic Differentiation and Autograd](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速了解Tensor\n",
    "\n",
    "pytorch作为NumPy的替代品，可以利用GPU的性能进行计算；可作为一个高灵活性、速度快的深度学习平台。\n",
    "\n",
    "Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。因此经常看到把numpy的数组包装为tensor再运算。tensor的操作和numpy中的数组操作类似，不再赘述，详见官网。下面列举一些简单例子。首先pytorch的导入是import torch，因为torch一直都是那个torch，一开始是别的语言写的，现在在python下，所以就叫pytorch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor是pytorch的基本数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(5)\n",
    "# 如果想要从tensor中获取到长度的int数据\n",
    "type(list(x.shape)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.2755e-39, 1.0561e-38, 9.5510e-39],\n",
       "        [1.0561e-38, 4.5919e-39, 4.2246e-39],\n",
       "        [1.0286e-38, 1.0653e-38, 1.0194e-38],\n",
       "        [8.4490e-39, 1.0469e-38, 9.3674e-39],\n",
       "        [9.9184e-39, 8.7245e-39, 9.2755e-39]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个 5x3 的矩阵, 未初始化的:\n",
    "x = torch.Tensor(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中的一些基本运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6500, 0.7761, 0.6299],\n",
       "        [0.4548, 0.3825, 0.7434],\n",
       "        [0.9794, 0.3191, 0.1690],\n",
       "        [0.7228, 0.0245, 0.5472],\n",
       "        [0.1111, 0.3976, 0.0551]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个随机初始化的矩阵:\n",
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate的操作使用torch.cat可以完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3823e+31, 2.3877e-38])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.Tensor(1)\n",
    "x2 = torch.Tensor(1)\n",
    "torch.cat((x1,x2),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到torch中的size也是torch中的类，包装了python的list，自然地，加减运算的对象也都是torch的tensor了。运算可以使用运算符，也可以使用函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8280, 1.6769, 1.0463],\n",
       "        [0.5390, 1.0687, 1.1929],\n",
       "        [1.6647, 0.6605, 1.1308],\n",
       "        [1.6171, 0.6700, 0.7880],\n",
       "        [1.0453, 1.3943, 0.8812]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加法\n",
    "y = torch.rand(5, 3)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8280, 1.6769, 1.0463],\n",
       "        [0.5390, 1.0687, 1.1929],\n",
       "        [1.6647, 0.6605, 1.1308],\n",
       "        [1.6171, 0.6700, 0.7880],\n",
       "        [1.0453, 1.3943, 0.8812]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8280, 1.6769, 1.0463],\n",
       "        [0.5390, 1.0687, 1.1929],\n",
       "        [1.6647, 0.6605, 1.1308],\n",
       "        [1.6171, 0.6700, 0.7880],\n",
       "        [1.0453, 1.3943, 0.8812]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a.add_(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy和tensor之间有很多类似的地方，比如索引，形状改变等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7761, 0.3825, 0.3191, 0.0245, 0.3976])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以用类似Numpy的索引来处理所有的张量！\n",
    "x[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([[-0.6274, -1.2660,  1.3102,  1.9275],\n",
      "        [-0.7556,  0.4957,  2.2412,  0.6251],\n",
      "        [-0.5370,  2.1314, -0.0755,  0.4169],\n",
      "        [-0.4280,  0.4309, -0.4394, -0.7138]])\n",
      "tensor([-0.6274, -1.2660,  1.3102,  1.9275, -0.7556,  0.4957,  2.2412,  0.6251,\n",
      "        -0.5370,  2.1314, -0.0755,  0.4169, -0.4280,  0.4309, -0.4394, -0.7138])\n",
      "tensor([[-0.6274, -1.2660,  1.3102,  1.9275, -0.7556,  0.4957,  2.2412,  0.6251],\n",
      "        [-0.5370,  2.1314, -0.0755,  0.4169, -0.4280,  0.4309, -0.4394, -0.7138]])\n"
     ]
    }
   ],
   "source": [
    "# 改变大小: 如果你想要去改变tensor的大小, 可以使用 torch.view:\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # -1就是让pytorch自己根据其他的维度去判断这里该是几维\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于和numpy的紧密联系，因此pytorch的张量和numpy数组可以很方便的转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意numpy的array和torch的tensor转换后，数据是绑定的，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy的array： [1. 1. 1. 1. 1.]\n",
      "array转为torch的tensor： tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 看改变 np 数组之后 Torch Tensor 是如何自动改变的\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(\"numpy的array：\",a)\n",
    "print(\"array转为torch的tensor：\",b)\n",
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用 .cuda 方法将 Tensors 在GPU上运行.\n",
    "# 只要在  CUDA 是可用的情况下, 我们可以运行这段代码\n",
    "if torch.cuda.is_available():\n",
    "    b = b.cuda()\n",
    "    print(b + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU上的所有张量(CharTensor除外)都支持与Numpy的相互转换。\n",
    "\n",
    "张量要在GPU上计算，需要主动从CPU移动到GPU上。张量可以使用.to方法移动到任何设备（device）上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9377])\n",
      "0.937736451625824\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "# 当GPU可用时,我们可以运行以下代码\n",
    "# 我们将使用`torch.device`来将tensor移入和移出GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # 直接在GPU上创建tensor\n",
    "    x = x.to(device)                       # 或者使用`.to(\"cuda\")`方法\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # `.to`也能在移动时改变dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU or CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查 cuda 设备是否可用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果有GPU的话 可以执行下面几个注释掉的代码\n",
    "# torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the current GPU memory usage by tensors in bytes for a given device\n",
    "# torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the current GPU memory managed by the caching allocator in bytes for a given device\n",
    "# torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何使用GPU？\n",
    "\n",
    "如果像存储数据到CPU，那么简单定义即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.DoubleTensor([1., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候变量在CPU上，且后续的运算都会在CPU上，为了将数据转移到GPU上，需要使用 .cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.FloatTensor([1., 2.]).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.cuda.FloatTensor([1., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候数据就在GPU上。模型同样可以转移到GPU上，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    "         nn.Linear(20, 20),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(20, 4),\n",
    "         nn.Softmax()\n",
    ")\n",
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查在不在GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有多个GPU，可以使用Data Parallelism，拆分数据，将 Data Generator 拆分到更小的mini batches，然后送到多个GPUs\n",
    "\n",
    "PyTorch中，并行通过torch.nn.DataParallel实现。一些关键函数：：\n",
    "\n",
    "- Replicate：Module在多个设备上复制。\n",
    "- Scatter：input在这些设备的第一维中分布。\n",
    "- Gather：从这些设备收集input和连接第一维。\n",
    "- parallel_apply：将从Scatter获得的一组输入分布式应用于从Replicate获得的相应的分布式Modules。\n",
    "\n",
    "```Python\n",
    "# Replicate module to devices in device_ids\n",
    "replicas = nn.parallel.replicate(module, device_ids)\n",
    "# Distribute input to devices in device_ids\n",
    "inputs = nn.parallel.scatter(input, device_ids)\n",
    "# Apply the models to corresponding inputs\n",
    "outputs = nn.parallel.parallel_apply(replicas, inputs)\n",
    "# Gather result from all devices to output_device\n",
    "result = nn.parallel.gather(outputs, output_device)\n",
    "```\n",
    "\n",
    "或者一种更简单的方式：\n",
    "\n",
    "```Python\n",
    "model = nn.DataParallel(model, device_ids=device_ids)\n",
    "result = model(input)\n",
    "```\n",
    "\n",
    "更多信息可以关注：\n",
    "\n",
    "- [Multi-GPU Framework Comparisons](https://medium.com/@iliakarmanov/multi-gpu-rosetta-stone-d4fa96162986)\n",
    "- [ilkarman/DeepLearningFrameworks](https://github.com/ilkarman/DeepLearningFrameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是Tensor的常见计算，日常积累。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本算术运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor是一种数据结构，是 PyTorch 的基本构建块。Tensors 非常像 numpy 数组，与 numpy 不同的是，张量旨在利用 GPU 的并行计算能力。许多 Tensor 语法类似于 numpy 数组的语法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "\n",
    "直接调用Tensor即可完成初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.Tensor(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和numpy不同的是，为了完成梯度下降等优化计算，Tensor还有梯度相关的属性（后续文本会有更多介绍），初始化时候requires_grad默认是False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_one = torch.ones(4)\n",
    "t_one.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires_grad具有传染性。这意味着当通过对其他Tensors运算创建一个Tensor，且其中至少有一个用于创建的张量requires_grad被设置为True时，运算结果Tensor的requires_grad也将被设置为True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((3,3), requires_grad = True)\n",
    "w1 = torch.randn((3,3))\n",
    "b = w1*a \n",
    "b.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要计算后的张量不被前序变量传染，可以使用detach()（非inplace运算），先detach()再计算即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_detach = w1*(a.detach())\n",
    "b_detach == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_detach.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detach()不是inplace运算，所以a还是有requires_grad的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 均值计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45003601 0.97489784 0.62049398 0.09110269]\n",
      " [0.40026833 0.85759564 0.68546586 0.27271109]\n",
      " [0.88953855 0.26980388 0.46891119 0.7821735 ]\n",
      " [0.07011137 0.85454264 0.09227025 0.40843787]\n",
      " [0.56613465 0.39923638 0.829733   0.04183137]\n",
      " [0.17818573 0.57736378 0.49219906 0.89792548]] (6, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.42571244, 0.65557336, 0.53151222, 0.415697  ]),\n",
       " array([6., 6., 6., 6.]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(6,4)\n",
    "print(X,X.shape)\n",
    "avg_np, _ = np.average(X, axis=0, returned=True)\n",
    "avg_np, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4257, 0.6556, 0.5315, 0.4157], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_th = torch.tensor(X)\n",
    "avg_th = torch.mean(X_th, dim=0)\n",
    "avg_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (X == X_th.numpy()).all()\n",
    "# assert (avg_np == avg_th.numpy()).all()   # this fails alread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求和计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求和运算和平均运算类似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta=torch.ones(5)\n",
    "tc=torch.sum(ta)\n",
    "tc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 幂次计算\n",
    "\n",
    "PyTorch下张量进行幂次计算需要使用 torch.pow 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1996,  0.6044, -1.0557, -0.4547])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4389, 0.3653, 1.1146, 0.2067])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4389, 0.3653, 1.1146, 0.2067])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(a, torch.Tensor([2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch中的广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试试torch的广播功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "t2=torch.sqrt(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8264, 0.8723, 0.8938],\n",
       "        [0.9070, 0.9162, 0.9231]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1/((t2+0.1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### element-wise运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta=torch.ones(5)\n",
    "tb=torch.zeros(5)\n",
    "ta-tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([2]).repeat(1,5)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z/ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td=ta/z\n",
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te=td**2\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = torch.arange(1., 5.)\n",
    "a = torch.arange(1., 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.,   4.,  27., 256.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(a, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看二维的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "t2=t1.repeat(1,4).view(-1, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [10., 11., 12.],\n",
       "        [10., 11., 12.],\n",
       "        [10., 11., 12.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.FloatTensor([[7,8,9],[10,11,12]])\n",
    "t4=t3.repeat(1,4).view(-1, 3)\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.],\n",
       "        [36., 36., 36.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SST = (t4 - t2) ** 2\n",
    "SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([288., 288., 288.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SST=torch.sum(SST,dim=0)\n",
    "SST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较大小\n",
    "\n",
    "两个张量，element-wise比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 4])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor((1, 2, -1))\n",
    "b = torch.tensor((3, 0, 4))\n",
    "torch.maximum(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果张量大小不同，也会默认执行广播运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor((1, 2, -1))\n",
    "b = torch.tensor([0])\n",
    "torch.maximum(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理NaN值\n",
    "\n",
    "有时候会有一nan值需要处理。在torch中检测是否有nan值可以使用：x != x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.nan==np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., nan,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5 = torch.FloatTensor([[7,np.nan,9],[10,11,12]])\n",
    "t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是直接判断一个多维张量是否有nan值，可以使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask= t5==t5\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please check\n"
     ]
    }
   ],
   "source": [
    "if len(mask[mask == True]) > 0:\n",
    "    print(\"please check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.,  9., 10., 11., 12.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t6=t5[mask]\n",
    "t6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让一般的数值计算能够成为梯度可追踪的计算，我们有时候需要将常见的一些计算用tensor重写，下面是NSE的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "def nse_2d(t,p):\n",
    "    seq_length = t.shape[0]\n",
    "    Ngage = t.shape[1]\n",
    "    tmean = torch.mean(t, dim=0)\n",
    "    tmeans = tmean.repeat(seq_length, 1)\n",
    "    SST = torch.sum((t - tmeans) ** 2, dim=0)\n",
    "    SSRes = torch.sum((t - p) ** 2, dim=0)\n",
    "    # Same as Fredrick 2019\n",
    "    # temp = SSRes / ((torch.sqrt(SST) + 0.1) ** 2)\n",
    "    # original NSE\n",
    "    temp = SSRes / SST\n",
    "    loss = torch.sum(temp) / Ngage\n",
    "    return loss\n",
    "\n",
    "t1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "t2 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "print(nse_2d(t1,t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对Tensor的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复制操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似numpy的repeat和tile，torch中可以使用repeat："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 1., 2., 3., 1., 2., 3., 1., 2., 3.],\n",
       "        [4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "z.repeat(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.repeat(1,4).view(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.repeat(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9436, -0.7028,  0.0103, -1.0038],\n",
       "         [ 0.5544,  0.4460, -1.0549, -0.4126],\n",
       "         [ 0.9392, -1.1895, -1.6707, -0.2637]],\n",
       "\n",
       "        [[ 1.1112,  0.8791,  2.0294,  0.6844],\n",
       "         [ 0.4926, -0.2633,  1.7268, -0.2183],\n",
       "         [-0.6889,  0.2896,  0.9341,  1.2021]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1112,  0.8791,  2.0294,  0.6844],\n",
       "        [ 0.4926, -0.2633,  1.7268, -0.2183],\n",
       "        [-0.6889,  0.2896,  0.9341,  1.2021]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1112,  0.8791,  2.0294,  0.6844],\n",
       "         [ 0.4926, -0.2633,  1.7268, -0.2183],\n",
       "         [-0.6889,  0.2896,  0.9341,  1.2021]],\n",
       "\n",
       "        [[ 1.1112,  0.8791,  2.0294,  0.6844],\n",
       "         [ 0.4926, -0.2633,  1.7268, -0.2183],\n",
       "         [-0.6889,  0.2896,  0.9341,  1.2021]],\n",
       "\n",
       "        [[ 1.1112,  0.8791,  2.0294,  0.6844],\n",
       "         [ 0.4926, -0.2633,  1.7268, -0.2183],\n",
       "         [-0.6889,  0.2896,  0.9341,  1.2021]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1,:,:].repeat(3,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与numpy之间的转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor 转换为numpy："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)    # see how the numpy array changed in value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意tensor变化时，array也会跟着变。反过来也一样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to NumPy and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 维度变换\n",
    "\n",
    "再看看维度变换，三维变二维："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.,  8.,  9.],\n",
       "         [10., 11., 12.]],\n",
       "\n",
       "        [[ 7.,  8.,  9.],\n",
       "         [10., 11., 12.]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.FloatTensor([[[7,8,9],[10,11,12]],[[7,8,9],[10,11,12]]])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [10., 11., 12.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2=t1.view(-1, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [10., 11., 12.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3=t1.reshape(-1, 3)\n",
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape 和 view 的区别这里简单补充下，参考了：[Pytorch-reshape与view的区别](https://congluwen.top/2018/12/pytorch_reshape_view/)。Pytorch中reshape()与view()都可以改变Tensor的shape但是有略微的区别.\n",
    "\n",
    "- view()只可以由torch.Tensor.view()来调用，view():\n",
    "    - 不改变Tensor数据，改变Tensor的size(即shape)\n",
    "    - 对于一个将要被view的Tensor，新的size必须与原来的size与stride兼容,即新的维度必须是以下两种情况: \n",
    "        - 是原有维度的一个子空间\n",
    "        - 只跨越原有满足邻接条件，stride[i]=stride[i+1]×size[i+1]的原有维度d,d+1,…,d+k\\\n",
    "    - 否则，在view之前必须调用contiguous()方法，对于该方法的讨论，见[StackOverflow](https://stackoverflow.com/questions/48915810/pytorch-contiguous)\n",
    "- reshape()可以由torch.reshape(),也可由torch.Tensor.reshape()调用。reshape():\n",
    "    - 同样也是返回与input数据量相同，但形状不同的tensor\n",
    "    - 若满足view的条件，则不会copy，若不满足，则会copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7203,  0.3192,  1.4156],\n",
       "        [-0.2007,  0.3821, -0.2626]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7203, -0.2007],\n",
       "        [ 0.3192,  0.3821],\n",
       "        [ 1.4156, -0.2626]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列表拼接\n",
    "\n",
    "使用torch.cat可以实现列表的拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_lst = [torch.randn(2, 3, 1), torch.randn(2, 3, 1)]\n",
    "torch.cat(tensor_lst, dim=-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  sparse update 操作\n",
    "\n",
    "主要基于 [pytorch_scatter](https://github.com/rusty1s/pytorch_scatter) 来实现 scatter散点 和 segment分段 等操作。散点和分段操作可以粗略地描述为基于给定的 \"组-索引\"张量的 reduce 操作。分段操作需要对 \"组-索引 \"张量进行排序，而散点操作则不受这些要求的限制。\n",
    "\n",
    "安装：\n",
    "\n",
    "```Shell\n",
    "conda install pytorch-scatter -c pyg\n",
    "```\n",
    "\n",
    "pytorch_scatter中包含的所有操作都是可广播的，能在不同的数据类型上工作，在CPU和GPU上都有相应的后向实现，并且是完全可追踪的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch_scatter import scatter\n",
    "\n",
    "src = torch.randn(10, 6, 64)\n",
    "index = torch.tensor([0, 1, 0, 1, 2, 1])\n",
    "\n",
    "# Broadcasting in the first and last dim.\n",
    "out = scatter(src, index, dim=1, reduce=\"sum\")\n",
    "\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的操作是类似下图这样的：\n",
    "\n",
    "![](img/add.svg)\n",
    "\n",
    "即有相同index号的一起执行同一种运算。\n",
    "\n",
    "另一种类似的操作 Segment COO：\n",
    "\n",
    "![](img/segment_coo.svg)\n",
    "\n",
    "通过顺序的编号，将张量切分成几个组别，比如下面的例子，顺序分成了三组 0和0第一组，接下来三个1是第二组，最后2一组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch_scatter import segment_coo\n",
    "\n",
    "index = torch.tensor([0, 0, 1, 1, 1, 2])\n",
    "index = index.view(1, -1)  # Broadcasting in the first and last dim.\n",
    "\n",
    "out1 = segment_coo(src, index, reduce=\"sum\")\n",
    "\n",
    "print(out1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一种类似的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch_scatter import segment_csr\n",
    "\n",
    "indptr = torch.tensor([0, 2, 5, 6])\n",
    "indptr = indptr.view(1, -1)  # Broadcasting in the first and last dim.\n",
    "\n",
    "out2 = segment_csr(src, indptr, reduce=\"sum\")\n",
    "\n",
    "print(out2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indptr是指定了 0-2，2-5，5-6 的三个范围。\n",
    "\n",
    "当然以上例子也都不是必须选择全部范围："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "indptr = torch.tensor([2, 5, 6])\n",
    "indptr = indptr.view(1, -1)  # Broadcasting in the first and last dim.\n",
    "\n",
    "out2 = segment_csr(src, indptr, reduce=\"sum\")\n",
    "print(out2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试试别的维度，比如第一个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "indptr1 = torch.tensor([2, 5, 6])\n",
    "out3 = segment_csr(src, indptr1, reduce=\"sum\")\n",
    "print(out3.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到如果一维数组直接指定，就是在dim=0上操作，而之前的indptr是二维数组，就是在第二维上操作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再试试第三维："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "indptr2 = indptr1.view(1, 1, -1)\n",
    "out4 = segment_csr(src, indptr2, reduce=\"sum\")\n",
    "print(out4.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不想要广播，各列的所取范围不同，那就得循环每列一个来算了，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.],\n",
       "         [ 2.,  3.],\n",
       "         [ 4.,  5.]],\n",
       "\n",
       "        [[ 6.,  7.],\n",
       "         [ 8.,  9.],\n",
       "         [10., 11.]],\n",
       "\n",
       "        [[12., 13.],\n",
       "         [14., 15.],\n",
       "         [16., 17.]],\n",
       "\n",
       "        [[18., 19.],\n",
       "         [20., 21.],\n",
       "         [22., 23.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "t1 = torch.Tensor(np.arange(24).reshape(4,3,2))\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标是最后一个维度根据索引做reduce操作，但是要求是最后一个维度第一组不做操作，第二组才做，然后是第一个维度的索引来reduce，但是第二维度上每个索引号不同，比如现在第二维度共三项，假设就是三种不同索引号，i1 = [0,1,3], i2 = [0,2,3], i3 = [0,3]，那么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 1., 20.]), tensor([3., 9.]), tensor([17.])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1 = torch.tensor([0,1,3])\n",
    "i2 = torch.tensor([0,1,2])\n",
    "i3 = torch.tensor([2,3])\n",
    "i_lst = [i1, i2, i3]\n",
    "\n",
    "segs = []\n",
    "for k in range(t1.shape[-1]):\n",
    "    if k > 0:\n",
    "        for j in range(t1.shape[1]):\n",
    "            tjk = t1[:,j,k]\n",
    "            segs.append(segment_csr(tjk, i_lst[j], reduce=\"sum\"))\n",
    "\n",
    "segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  7., 13., 19.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[:,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  9., 15., 21.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[:,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 11., 17., 23.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[:,2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核对下，可以发现没有问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
