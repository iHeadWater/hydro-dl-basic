{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迁移学习介绍\n",
    "\n",
    "对于 pub 问题 等，迁移学习是个不错的方法，因此这里了解下迁移学习的基本方法。首先可以简要回顾下深度学习的相关概念：[深度学习入门者必看：25个你一定要知道的概念](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247485927&idx=1&sn=606f87b569a0eb7c29f1c2374ff698dd&chksm=e8d3be95dfa43783ddbd577ce8cc7db64c1b02bb55480c1d3f2ba8e06e4b948d1fd6b8e2defc&scene=21#wechat_redirect)\n",
    "\n",
    "A survey on Transfer Learning 是一篇迁移学习方面引用较多的文章。在这篇文章之前，先参考一些blog 记录关于迁移学习的基础知识：\n",
    "\n",
    "- [Transfer Learning - Machine Learning's Next Frontier](https://ruder.io/transfer-learning/)\n",
    "- [深度 | 迁移学习全面概述：从基本概念到相关研究（基本算是上一篇的翻译）](https://www.sohu.com/a/130511730_465975)\n",
    "- [什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？](https://www.zhihu.com/question/41979241)\n",
    "- [独家 | 一文读懂迁移学习（附学习工具包） ](https://www.sohu.com/a/156932670_387563)\n",
    "- [一文看懂迁移学习：怎样用预训练模型搞定深度神经网络？](https://zhuanlan.zhihu.com/p/27657264)\n",
    "- [模型汇总-14 多任务学习-Multitask Learning概述](https://zhuanlan.zhihu.com/p/27421983)\n",
    "- [A Gentle Introduction to Transfer Learning for Deep Learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n",
    "- [Transfer Learning in Keras with Computer Vision Models](https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/)\n",
    "- [An Introduction to Transfer Learning in Machine Learning](https://medium.com/kansas-city-machine-learning-artificial-intelligen/an-introduction-to-transfer-learning-in-machine-learning-7efd104b6026)\n",
    "\n",
    "另外，补充几篇paper做参考：\n",
    "\n",
    "- [A Survey on Transfer Learning](https://ieeexplore.ieee.org/abstract/document/5288526)\n",
    "- [Chapter 11: Transfer Learning --> torrey.handbook09.pdf](ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group)\n",
    "- [How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)\n",
    "- [Learning Transferable Features with Deep Adaptation Networks](https://arxiv.org/abs/1502.02791)\n",
    "- [Unsupervised Domain Adaptation with Residual Transfer Networks](http://papers.nips.cc/paper/6110-unsupervised-domain-adaptation-with-residual-transfer-networks)\n",
    "\n",
    "## 为什么需要迁移学习\n",
    "\n",
    "在监督学习方面，在过去几年中已经获得了训练越来越准确的模型的能力。这些成功的模型都是极其地重视数据的，依靠大量的标签数据来实现它们的性能。但是把机器学习的模型应用在自然环境中时，模型会面临大量之前未曾遇到的条件，它不知道如何去处理；另外，每一个用户都有他们自己的偏好，也需要处理和生成不同于之前用来训练的数据；还有有时会要求模型执行很多和训练相关但是不相同的任务。在所有这些情况下，会遭遇明显的表现下降甚至完全失败。\n",
    "\n",
    "更准确地讲，传统机器学习通常有两个基本假设，即**训练样本与测试样本满足独立同分布的**假设和**必须有足够可利用的训练样本**假设。然而，现实生活中这两个基本假设有时往往难以满足，因为现实世界是messy的，并且包含了无限多的场景。比如，股票数据的时效性通常很强，利用上个月数据训练出来的模型，往往很难顺利地运用到下个月的预测中去；比如公司开设新业务，但愁于没有足够的数据建立模型进行用户推荐。\n",
    "\n",
    "近年来在机器学习领域受到广泛关注的迁移学习恰恰解决了这两个问题。迁移学习用已有的知识来**解决目标领域中仅有少量有标签样本数据甚至没有数据的学习问题**，从根本上放宽了传统机器学习的基本假设。由于被赋予了人类特有的举一反三的智慧，迁移学习能够**将适用于大数据的模型迁移到小数据上，发现问题的共性，从而将通用的模型迁移到个性化的数据上，实现个性化迁移**。\n",
    "\n",
    "迁移学习(Transfer learning) 顾名思义就是就是**把已学训练好的模型参数迁移到新的模型来帮助新模型训练**。考虑到大部分数据或任务是存在相关性的，所以通过迁移学习我们可以**将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型**从而加快并优化模型的学习效率不用像大多数网络那样从零学习。\n",
    "\n",
    "Transfer Learning 本身是机器学习的一个分支，其初衷是节省人工标注样本的时间，让模型可以通过已有的标记数据（source domain data）向未标记数据（target domain data）迁移。比如下图所示，假设前两张正对着摄像机的行人作为训练集，后两张背对着的行人图片作为测试集，结果该模型的测试评分会很差，因为训练时没有考虑到摄像机观察角引起的问题，相类似在图像识别领域会有很多因素会降低识别率（例如光照，背景等）。ok，那能否用一些未标记的图片（类似图3，4这样的图），增强我们的行人检测模型，让它不仅可以识别正对着的行人，还可以识别背对着的行人？这就是迁移学习要干的事。\n",
    "\n",
    "![](v2-e92213c12444fc75ec3e9f5514e1ac28_hd.jpg)\n",
    "\n",
    "它的很多方法是不需要用神经网络的，而现在之所以Transfer Learning和神经网络联系如此紧密，主要因为NN的发展太快，太强大，导致 Transfer Learning 的研究都往神经网络靠了。因为深度 NN 是一个通过pre-train获得数据的分层特征表达，然后用高层语义分类的模型。模型底层是低级语义特征（比如说，边缘信息，颜色信息等），这样的特征实际上在不同的分类任务中都是不变的，而**真正区别的是高层特征**。这也解释了通常使用新的数据集去**更新** AlexNet，GoogleNet的**最后几层网络权值**，来实现简单的“迁移”这个小trick。和深度神经网络相结合的transfer模型基本思想都类似(为了保证域差异最小，loss项都会加一个MMD约束)。\n",
    "\n",
    "总之，迁移学习可以帮助我们处理全新的场景，它是机器学习在**没有大量标签数据的任务和域**中规模化应用所必须的。另外，非监督学习和强化学习虽然有很多前沿成果，但是在工业界，还存在着瓶颈。而迁移学习可以很快做出一些有影响的成果，low-hanging fruit。\n",
    "\n",
    "## 什么是迁移学习\n",
    "\n",
    "比如，在机器学习的经典监督学习场景中，如果我们要针对一些任务和域 A 训练一个模型，我们会假设被提供了针对同一个域和任务的标签数据，如下图所示。\n",
    "\n",
    "![](bc62ca0bdd3e4ac78e72c850e3143246_th.jpeg)\n",
    "\n",
    "现在我们可以在这个数据集上训练一个模型 A，并期望它在同一个任务和域中的未知数据上表现良好。\n",
    "\n",
    "另一个场景，当给定一些任务或域 B 的数据时，我们还需要可以用来训练模型 B 的有标签数据，这些数据要属于同一个任务和域，这样我们才能预期能在这个数据集上表现良好。\n",
    "\n",
    "比如我们的目标是训练识别夜间行人的模型，我们可以训练识别白天行人的，但是我们如果直接拿白天训练的结果用到夜晚，那么效果可能是很差的。因为模型继承了训练数据的偏差，它不知道怎么将模型泛化到新的领域。如果我们检测的是自行车，那么源模型是完全不能用的。\n",
    "\n",
    "当我们没有足够的来自于我们关心的任务或域的标签数据来训练可靠的模型时，传统的监督学习范式就支持不了了。\n",
    "\n",
    "迁移学习允许我们通过借用已经存在的一些相关的任务或域的标签数据来处理这些场景。我们尝试着把在源域中解决源任务时获得的知识存储下来，并将其应用在我们感兴趣的目标域中的目标任务上去。如下图所示：\n",
    "\n",
    "![](transfer_learning_setup.png)\n",
    "\n",
    "这里先提一个概念－－domain adaptation， 在研究source domain和target domain时，基于某一特征，会发现两个domain的数据分布差别很大，如下图所示：\n",
    "\n",
    "![](v2-8da69cb2647edf0ce726fa5391837e78_hd.jpg)\n",
    "\n",
    "红线是source dataset的颜色信息值分布，蓝线是target dataset的颜色值分布。很明显对于这一特征来讲，两个域的数据本来就是有shift的。而这个shift导致我们evaluate这个模型的时候准确率会大大降低。\n",
    "\n",
    "既然这个特征不合适，那我们就**换特征**，domain adaptation旨在利用各种的**feature transformation**手段，**学习一个域间不变的特征表达**，基于这一特征，我们就可以更好的同时对两个域的数据进行分类了。\n",
    "\n",
    "接下来根据文章A Survey on Transfer Learning，记录transfer learning 具体定义。不过记录定义之前，需要先补充一点关于文档分类的背景知识，主要参考了[数学之美](https://www.google.com/search?q=site%3Achina.googleblog.com%20%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E)，[利用朴素贝叶斯算法进行文档分类](https://www.jianshu.com/p/364887de2039)等，因为后面文章作者的举例都是关于文档分类的。\n",
    "\n",
    "文档分类或者新闻分类，就是把相似的新闻放到一类中。这要求设计一个算法能算出两篇新闻之间的相似性。所以需要用一组数字或一个向量来描述一篇新闻。\n",
    "\n",
    "一篇新闻有很多字，怎么将这些字转换成数字？以中文为例，对于一句话，不是一个字一个字的理解的，而是一个词一个词作为最小的意义单元的，各类语言统计模型都是建立在词的基础上的，因此首要解决的问题是中文分词。这是一个专业问题，这里不多记录，总之能用一些统计语言模型将句子进行分词。那么这些词如何数字化？通常是用词频 term frequency 来表达（背景故事可见[数学之美 系列九 -- 如何确定网页和查询的相关性](https://china.googleblog.com/2006/06/blog-post_3066.html)），即一个词在文中出现的次数/文章词数。另外，为了避免“的”这样的词太突出，突出更有主题性的词，就加上权重，权重的计算采用的是逆文本频率指数，log(D/Dw)，Dw表示一个词w在Dw个网页中出现过，D是全部网页数，也就是出现的越多，主题性越差，权重越小。这样通过TF和其权重IDF（TF/IDF，注意/不是除法的意思，只是表示在一起时起个分隔符的作用）的概念，就能将每个词都数字化了。\n",
    "\n",
    "数字化之后，就可以计算新闻之间的相似性了，最简单的当属余弦定理了。比如一个新闻，分词之后有1000个词，语料库中提取的所有词（词典）的总数是64000，那么TF/IDF计算后，就成为了64000个数组成的一个向量（也就是说，这个向量大部分元素是0的）。这个向量就是新闻的特征向量了。两个向量余弦定理计算，越相关，余弦愈大。当然这种方法是比较原始的，如果文章很多，要去分类，那就需要改进了，这是专业的领域了，这里就只记录下例子的逻辑，比如训练的语料库里总共有 13180 个文档，其中分成 20 个类别。词典总共有 130274 个词语，那么 X_train 是一个维度为 13180 x 130274 的稀疏矩阵（这是一个巨稀疏的矩阵）。y_train 的维度就是 13180 x 1。这样去训练即可。\n",
    "\n",
    "除了上述内容外，下面还提到了边缘概率分布，多个变量联合分布中的一个变量的边缘概率分布我们都学过，不过随机变量到随机向量到随机矩阵之后，可能就不那么清楚了。因此，也稍微补充点背景知识。这部分参考了：[从随机变量到随机向量再到随机矩阵：那个你不一定知道的矩阵高斯分布](https://zhuanlan.zhihu.com/p/26286575)。\n",
    "\n",
    "随机变量本身并不是一个普通的变量，更准确地说是一个函数，而概率是一种测度，是一个随机变量的结果，一个基于概率测度的结果（这里我也只能感觉感觉，准确定义就不多记录了，暂不需要）。随机变量的可测空间是实数空间的，$E = \\mathbb{R}$，而随机向量就是多元随机变量了，其可测空间就是$E = \\mathbb{R}^n$了，更简单地说，就是一列随机变量排排站，构成一个向量，**这里所谓的“排排站”对于一般没有任何限制的随机变量来说自然是没有限制，“排排站”就是排排站，但是若是对于有高斯分布限制的随机变量结果就是按照高斯分布的方式排排站喽，也就是整个向量要满足多维高斯分布**。边缘概率分布就是只取一个变量，在该变量取各个值时，其他所有变量概率之和。\n",
    "\n",
    "直观看看随机变量高斯分布和多维高斯分布之间的关系。\n",
    "\n",
    "- 随机变量$X \\sim \\mathcal{N}(\\mu,\\sigma ^2)$，说明这个随机变量的平均（期望）情况被均值$\\mu$所刻画，而这个随机变量的波动情况被$\\sigma ^2$刻画。\n",
    "- 随机向量属于高斯分布 $Y=[X_1, \\dots X_n]^T \\sim \\mathcal{N}(\\mu,\\Sigma)$，说明这个随机向量的每个分量的平均（期望）情况被对应均值向量 $\\mu$ 对应的分量所刻画，而分量之间的关系（包括自身和自身的关系）被后面的协方差函数所刻画。\n",
    "\n",
    "再扩展到随机矩阵，就是矩阵值都是随机变量。可测空间现在就是：$E=\\mathbb{R}^{n*n}$。对于它来说，均值矩阵要有：对应的随机矩阵的各个元素的平均（期望）情况都要被均值矩阵中对应的元素所刻画。 协方差矩阵则要去刻画各个分量之间的关系，但是问题来了，对于一个向量，两两之间的关系用一个矩阵就可以刻画了，但是现在是一个矩阵，而就是说有d个列向量，此时，我们应该如何刻画这种关系呢？现在需要两个矩阵，记为列协方差矩阵（column covariance matrix）和行协方差矩阵（row covariance matrix）。对一个随机矩阵，每列是一个随机向量，那么可以用对应的随机向量的协方差函数刻画，就是列协方差矩阵，对于每行，也是类似的，就是行协方差矩阵。那么矩阵高斯分布就是这样的形式：$\\mathcal{N}(M,\\Sigma,\\Omega)$，其概率分布表达比较复杂，就不写了。这个矩阵的边缘分布，就有行边缘分布和列边缘分布了。不多说了，可以参考参考博客。\n",
    "\n",
    "以上简单补充点背景知识，现在回到本文内容，迁移学习涉及到**域 domain 和 任务 task**的概念。\n",
    "\n",
    "一个域 $\\mathcal{D}$ 由一个特征空间 $\\mathcal{X}$ 和特征空间上的边缘概率分布 $P(X)$ 组成，其中，$X=\\{x_1,\\dots x_n\\} \\in \\mathcal{X}$。以文档分类为例，每个term都是一个binary feature，那么 $\\mathcal{X}$ 是所有term向量的空间，$x_i$ 是对应于某文档的第 i 个term向量，$X$ 是一个特定的学习样本。这里稍微再白话解释下，就是说一个文章就是一个$x_i$；一个文章有很多词（英文的话就直接一个词，中文的话就先分词），一个词一个term，一个文章就是一个term的向量；一个X就是一个样本，里面有很多文章，所有文章对应的term向量一起就组成了特征空间$\\mathcal{X}$。通常如果两个domain不同，那么它们就有不同的特征空间或不同的边缘概率分布。这里边缘分布就直接当作多个term向量（当作一个变量）联合分布的边缘概率分布理解就行了。\n",
    "\n",
    "给定一个 domain $\\mathcal{D}=\\{\\mathcal{X},P(X)\\}$，一个 task 由一个 label 空间 $\\mathcal{y}$ 及一个目标预测函数 $f(\\cdot)$ 组成，可表示为 $\\mathcal{T}=\\{\\mathcal{y},f(\\cdot)\\}$，该预测函数是不能观察到的，但是可以从训练数据中学习得到，训练数据是由特征—标签对 $\\{x_i \\in X,y_i \\in \\mathcal{y}\\}$ 组成的。函数$f(\\cdot)$可以用来预测一个新实例 $x$ 对应的label $f(x)$ 。从概率的角度，$f(x)$可以表示为$P(y|x)$。在文档分类例子中，$\\mathcal{y}$是所有标签的集合，对于一个二分类任务就是True，False，$y_i$要么是“True”，要么是“False”。\n",
    "\n",
    "为了简化，这里只考虑一个source domain $\\mathcal{D}_S$，和一个target domain $\\mathcal{T}_S $的例子。更具体的，表示source domain data 为：$D_S = \\{(x_{S_1},y_{S_1}),\\dots,(x_{S_{n_S}},y_{S_{n_S}})\\}$，其中， $x_{S_i}\\in \\mathcal{X}_S$，是数据实例data instance，$y_{S_i}\\in \\mathcal{Y}_S$ 是对应的class label类标签。在文档分类例子中，$\\mathcal{D}_S$ 是 term vectors 和它们对应的true 或 false的class labels的集合。类似地，对target-domain 数据，可以表示为：$D_T = \\{(x_{T_1},y_{T_1}),\\dots,(x_{T_{n_T}},y_{T_{n_T}})\\}$，其中，输入时$x_{T_i}\\in \\mathcal{X}_T$，对应输出为$y_{T_i}\\in \\mathcal{Y}_T$。在多数情况下，$0 \\le n_T \\le n_S$，即大多数情况下，目标任务的例子有限，比源数据少。\n",
    "\n",
    "现在可以定义transfer learning了：\n",
    "\n",
    "给定一个source domain $\\mathcal{D}_S$ 和一个learning task $\\mathcal{T}_S$，以及一个target domain $\\mathcal{D}_T$ 和一个learning task $\\mathcal{T}_T$，迁移学习的目标就是用从$\\mathcal{D}_S$和$\\mathcal{T}_S$（$\\mathcal{D}_S \\neq \\mathcal{D}_T$ 或 $\\mathcal{T}_S \\neq \\mathcal{T}_T$）获取的知识来帮助提升$\\mathcal{D}_T$中目标预测函数$f_T(\\cdot)$的学习。\n",
    "\n",
    "在上述定义中，domain是 $\\mathcal{D} = \\{ \\mathcal{X},P(X) \\}$。 条件$\\mathcal{D_S} \\neq \\mathcal{D_T}$ 意味着 $\\mathcal{X}_S \\neq \\mathcal{X}_T$或者$P_S(X) \\neq P_T(X)$。比如，在文档分类中，这意味着，在 一个source document 集和 一个target document 集间，它们的term features不同 或者 他们的边缘分布不同。\n",
    "\n",
    "类似地，task定义为 $\\mathcal{T} = \\{\\mathcal{y}, P(Y|X)\\}$。那么，条件 $\\mathcal{T}_S \\neq \\mathcal{T}_T$ 意味着 $\\mathcal{y}_S \\neq \\mathcal{y}_T$ 或者  $P(Y_S|X_S) \\neq P(Y_T|X_T)$ 。当target和source domains 相同，比如$\\mathcal{D}_S = \\mathcal{D}_T$，并且它们的learning tasks 相同，比如$\\mathcal{T}_S \\neq \\mathcal{T}_T$，学习问题就变为一个传统的机器学习问题了。当domains不同，那么：1）domain之间的feature spaces 不同，比如$\\mathcal{X}_S \\neq \\mathcal{X}_T$ 2）domains之间的feature spaces相同，但是domain data的边缘分布不同，比如$P(X_S) \\neq P(X_T)$，其中，$X_{S_i} \\in \\mathcal{X}_S$ , $X_{T_i} \\in \\mathcal{X}_T$。在文档分类例子中，case1对应两个documents的集合用不同的语言表示的时候，case2可能对应source domain 文档和target domain的文档关注不同的主题。\n",
    "\n",
    "给定特定的domains $\\mathcal{D}_S$ 和 $\\mathcal{D}_T$，当learning tasks $\\mathcal{T}_S$ 和 $\\mathcal{T}_T$ 不同时，那么 1）domains间label spaces 不同，比如 $\\mathcal{y}_S \\neq \\mathcal{y}_T$ 或者 2）domains的条件概率分布不同，比如 $P(Y_S|X_S) \\neq P(Y_T|X_T)$，其中$Y_{S_i} \\in \\mathcal{y}_S$ ，$Y_{T_i} \\in \\mathcal{y}_T$。在文档分类例子中，case1对应的场景是source domain有binary 文档类，而target domain有10个类型要给文档分。case2对应的情景是source和target文档在用户定义的类别方面非常不平衡。\n",
    "\n",
    "此外，当两个domains的feature spaces之间存在显式或者隐式的关系时，我们说source 和 target domains是related。\n",
    "\n",
    "## 迁移学习的类型\n",
    "\n",
    "在迁移学习中，有以下三个主要问题：\n",
    "\n",
    "1. what to transfer：哪部分知识可以迁移。共性的知识是可以迁移的，怎么判断哪些是共性知识；\n",
    "2. how to transfer：确定可以迁移的部分之后，怎么迁移，使用什么样的算法；\n",
    "3. when to transfer：在什么情况下，可以用迁移技术。如果有些不适合使用迁移技术而强行迁移的话会导致 negative transfer。\n",
    "\n",
    "迁移学习有以下几种情况：\n",
    "\n",
    "|Learning settings|source and target domains|source and target tasks|\n",
    "|-|-|-|\n",
    "|Inductive transfer learning|same|different but related|\n",
    "|Unsupervised transfer learning|different but related|different but related|\n",
    "|Transductive transfer learning|different but related|same|\n",
    "\n",
    "每种情况又可以继续细分出不同的情境：\n",
    "\n",
    "|Transfer Learning settings|Related Areas|source domain labels|target domain labels|tasks|\n",
    "|-|-|-|-|-|\n",
    "|Inductive transfer learning|multi-task learning|available|available|Regression,Classification|\n",
    "||self-taught learning|unavailable|available|Regression,Classification|\n",
    "|Transductive transfer learning|Domain Adaptation, Sample Selection Bias, Co-variate Shift|available|unavailable|Regression,Classification|\n",
    "|Unsupervised transfer learning||unavailable|unavailable|Clustering, Dimensionality Reduction|\n",
    "\n",
    "![](Picture2.png)\n",
    "\n",
    "- Inductive transfer learning: 不论源域和目标域是否相同，目标任务和源任务都不同。这时候，目标域的一些标记数据要来induce一个预测模型$f_T(\\cdot)$。此外，根据源域中有标签和无标签数据的不同情景，又可以把indcutive transfer learning 进一步分为两种情况：\n",
    "    - multi-task learning：在源域中有许多标记数据。这时候inductive transfer learning和多任务学习的设定相似。不过inductive transfer learning目的在于通过从源域迁移知识以在目标域获得更好的性能，而多任务学习则试图同时学习源任务和目标任务。\n",
    "    - self-taught learning：源域没有标记数据可用，这时候inductive transfer learning就和自学习相似。在自学习情境下，源域和目标域的标记空间可能不同，这意味着源域的side information不能直接拿来用。这时候就和源域没有标记数据可用的inductive transfer learning是一样的了。\n",
    "- transductive transfer learning:源目标和任务目标相同，而源域和目标域不同。这时候，目标域没有标记数据可用，而源域有很多标记数据可用（个人注：这种情况可能是水文PUB问题可以借鉴的）。此外，根据源域和目标域的不同情境，可以进一步将transductive transfer learning分为两种情况“\n",
    "    - 源域和目标域的特征空间不同，$\\mathcal{X}_S \\neq \\mathcal{X}_T$\n",
    "    - 源域和目标域的特征空间相同，$\\mathcal{X}_S = \\mathcal{X}_T$，但是输入数据的边缘概率分布不同，$P(\\mathcal{X}_S) \\neq P(\\mathcal{X}_T)$\n",
    "- unsupervised transfer learning:和inductive transfer learning 相似，目标任务和源任务不同但是相关。不过unsupervised transfer learning关注于解决目标域的非监督学习任务，比如聚类，降维，和密度估计。这种情况下，源域和目标域都是没有标记数据的。\n",
    "\n",
    "以上不同情境可以根据what to transfer总结为四种情境：\n",
    "\n",
    "|Transfer Learning Approaches|Brief Description|\n",
    "|-|-|\n",
    "|Instance-transfer|To re-weight some labeled data in the source domain for use in the target domain|\n",
    "|Feature-representation-transfer|Find a \"good\" feature representation that reduces difference between the source and the target domains and the error of classification and regression models|\n",
    "|Parameter-transfer|Discover shared parameters or priors between the source domain and target domain models, which can benefit for transfer learning|\n",
    "|Relational-knowledge-transfer|Build mapping of relational knowledge between the source domain and the target domains. Both domains are relational domains and i.i.d assumption is relaxed in each domain|\n",
    "\n",
    "- 第一种 Instance-based transfer learning 方法假设源域的数据的某些部分**通过reweighting能在目标域复用**。Instance reweighting 和 importance sampling 是两种主要的技术；\n",
    "- 第二种是feature-representation-transfer 方法，此法的核心idea在于**为目标域学习一个好的特征表示**。这里，用来跨域迁移的知识被编码到被学习的特征表示中。有了新的特征表达，目标任务的性能可以显著提升；\n",
    "- 第三种是parameter-transfer 方法，它假设**源任务和目标任务共享模型超参数的先验分布**。迁移的知识可以编码为共享参数或先验。这样，通过发现共享参数，知识可以跨任务学习；\n",
    "- 最后是relational-knowledge-transfer 问题，它处理相关域的迁移学习，其基本假设是源域和目标域的 数据中关系 是相似的。因此，**数据间关系的知识是可以迁移的**。比如统计关系学习技术。\n",
    "\n",
    "上面两种分法的具体情景关系如下表所示：\n",
    "\n",
    "||Inductive transfer learning|transductive transfer learning|unsupervised transfer learning|\n",
    "|-|-|-|-|\n",
    "|Instance-transfer|$\\surd$|$\\surd$||\n",
    "|Feature-representation-transfer|$\\surd$|$\\surd$|$\\surd$|\n",
    "|Parameter-transfer|$\\surd$|||\n",
    "|Relational-knowledge-transfer|$\\surd$|||\n",
    "\n",
    "通过上图，上表以及各段文字，结合自己的专业背景，有几个问题可能和迁移学习相关。\n",
    "\n",
    "首先要利用迁移学习的话也要从source domain有标记数据的开始，这样会相对好做一些，所以非监督学习的迁移学习和Inductive transfer learning中的Case1可以先不看。进一步地，考虑径流预报问题，对于小样本的国内有标记数据的回归问题，可以利用Inductive transfer learning中的**case2**来处理，即**多任务学习**这方面的算法是可以稍微认真了解下的。另外，对于PUB问题，即目标域没有标记的径流预报，可以借鉴transductive transfer learning方法，这方面可以结合其他应用类型的文献来帮助了解。\n",
    "\n",
    "再结合迁移学习的第二种分类方法看，个人认为  Instance-transfer，Feature-representation-transfer，Parameter-transfer，Relational-knowledge-transfer 这几个方法都是可能用得到的。\n",
    "\n",
    "接下来，更详细地看看几类不同情境下的学习。\n",
    "\n",
    "### Inductive transfer learning\n",
    "\n",
    "一个更详细的定义：给定源域$\\mathcal{D}_S$和任务$\\mathcal{T}_S$，以及目标域$\\mathcal{D}_T$和目标任务$\\mathcal{T}_T$。Inductive transfer learning 旨在使用$\\mathcal{D}_S$和$\\mathcal{T}_S$中的知识，以帮助提升$\\mathcal{D}_T$中目标预测函数$f_T(\\cdot)$的学习，其中$\\mathcal{T}_S \\neq \\mathcal{T}_T$。\n",
    "\n",
    "基于上述定义，目标域需要一些标记数据来作为训练数据推求目标域预测函数。如前所述，这里有两种cases，一是源域的标记数据可用；二是源域的标记数据不可用但是无标记数据可用。大多数迁移学习关注前者。\n",
    "\n",
    "#### Transferring Knowledge of Instances\n",
    "\n",
    "instance-transfer 方法可以这样直观理解：虽然源域的数据不能直接复用，但数据仍有部分可以和目标域的标记数据一起复用。比如 TrAdaBoost 算法，一种AdaBoost 的扩展，来解决 Inductive transfer learning问题。TrAdaBoost假设源域和目标域数据使用相同的特征和标记集，但是两个域的数据分布不同。除此之外，TrAdaBoost 假设由于源域和目标域的分布的差别，源域数据的一部分会在目标域的学习中有用，但有些可能没用，甚至有反作用。TrAdaBoost试图**迭代重新赋权以减小bad 的源数据，加强good源数据**，以使对目标域更有用。迭代的每一轮，TrAdaBoost 会在带权的源和目标数据训练base 分类器，仅计算目标数据上的error，更多地，TrAdaBoost 使用和 AdaBoost 相同的策略来更新目标域不正确的分类样本，但同时使用与 AdaBoost 不同的策略来更新不正确的源分类样本。还有其他方法见原文。\n",
    "\n",
    "#### Transferring Knowledge of Feature Representations\n",
    "\n",
    "Inductive transfer learning 中的feature-representation-transfer 方法旨在找到good representations来最小化域的divergence和分类或回归模型误差。找到good 特征表示的策略在不同类型源域数据中不同。如果源域中很多标记数据可用，监督学习方法可以用来构建一个特征表示。这和多任务学习中的 common feature learning 很像。如果源域中没有标记数据可用，那么可以使用非监督学习算法来构建特征表示。\n",
    "\n",
    "##### Supervised Feature Construction\n",
    "\n",
    "Inductive transfer learning 的有监督特征构建方法和多任务学习中的相似。基本思想是**学习一个能在相关任务间共享的低维表示**。除此之外，学习到的新的特征可以减少分类或回归的每个任务的误差。common features 可以通过求解优化问题来获取：\n",
    "\n",
    "![](Picture1.png)\n",
    "\n",
    "简单理解下上面的公式，$U^T$是针对目标域的降维矩阵，$a_t$是参数，所以上式是最小化$U^T$对源和目标域的$x_{t_i}$降维加权后得到的结果的损失（后面$||A||$范式是正则化？可能吧）。有不同的方法来求解，这里就不表述了。最后求出A,U，得到降维表征和参数值。\n",
    "\n",
    "##### Unsupervised Feature Construction\n",
    "\n",
    "有稀疏编码的方法可以作为一种学习高级特征的迁移学习的非监督学习方法。先在源域通过优化计算学习高级特征基向量b，然后第二步基于这个基向量b在目标域中学习高级特征。因为个人可能暂时用不到这个，所以就不多记录了。\n",
    "\n",
    "#### Transferring Knowledge of Parameters\n",
    "\n",
    "Inductive transfer learning 中参数迁移的方法可以借鉴多任务学习中的方法，因为迁移学习和多任务学习在源域和目标域的学习上是相似的，区别在于迁移学习的目标是让目标域的结果更好，而多任务学习是让源和目标同时更好。所以在Inductive transfer learning 参数迁移方法中，可以为目标域的loss 函数分配更高的权重来确保在目标域我们可以得到更好的性能。\n",
    "\n",
    "多任务学习中的算法有基于高斯过程的MT-IVM法，高斯过程和层级贝叶斯框架结合的方法等，都可以借鉴，后面到具体实例用到时候再进一步学习。\n",
    "\n",
    "#### Transferring Relational Knowledge\n",
    "\n",
    "和前面几个方法不同，此方法在relational domains处理迁移学习，数据是非独立同分布的，并且能被多种关系表示，比如网络数据和社交网络数据。此方法不假设每个域的数据是独立同分布的。它试图迁移数据间关系。这里用到的技术是统计相关学习技术。比如TAMAR。\n",
    "\n",
    "### TRANSDUCTIVE TRANSFER LEARNING\n",
    "\n",
    "给定源域－源任务和目标域－目标任务，transductive transfer learning目标是运用在源域上学习的知识来提升目标预测函数，源任务和目标任务需要是相同的任务。如前所示，这里有两个cases，一是源域和目标域的特征空间不同，二是输入数据的边缘概率分布不同。更多方法是针对第二种case表述的。\n",
    "\n",
    "#### Transferring the Knowledge of Instances\n",
    "\n",
    "transductive transfer learning 中更多的 instance-transfer 方法是由importance sampling方法启发的。这种方法的了解可从empirical risk minimization 问题开始。\n",
    "\n",
    "最小化风险期望：\n",
    "$$\\theta ^* = arg min_{\\theta \\in \\Theta} \\mathbb E _{(x,y)\\in P} [l(x,y,\\theta)]$$\n",
    "\n",
    "因为期望可能不好算，所以有最小化ERM：\n",
    "$$\\theta ^* = arg min_{\\theta \\in \\Theta} \\frac{1}{n}\\sum_{i=1}^n [l(x,y,\\theta)]$$\n",
    "\n",
    "在transductive transfer learning情境下，我们想要通过最小化期望风险学习一个目标域的优化模型：\n",
    "$$\\theta ^* = arg min_{\\theta \\in \\Theta} \\sum_{(x,y) \\in D_T}P(D_T)l(x,y,\\theta)$$\n",
    "\n",
    "然而，因为目标域训练数据中没有标记数据，因此我们必须要从源域数据中学习一个模型出来。如果$P(D_S) = P(D_T)$，那么我们能很简单的从下式优化计算中求解：\n",
    "$$\\theta ^* = arg min_{\\theta \\in \\Theta} \\sum_{(x,y) \\in D_S}P(D_S)l(x,y,\\theta)$$\n",
    "\n",
    "但$P(D_S) \\neq P(D_T)$时，我们需要修改以上优化问题为如下式子：\n",
    "$$\\theta ^* = arg min_{\\theta \\in \\Theta} \\sum_{(x,y) \\in D_S} \\frac{P(D_T)}{P(D_S)}P(D_S)l(x,y,\\theta) \\approx arg min_{\\theta \\in \\Theta} \\sum_{i=1}^{n_S} \\frac{P(x_{T_i},y_{T_i})}{P(x_{S_i},y_{S_i})}l(x_{S_i},y_{S_i},\\theta)$$\n",
    "\n",
    "通过权重来给损失值惩罚来为目标域学习一个模型。又因为源域和目标域的条件概率相同，所以有：\n",
    "$$\\frac{P(x_{T_i},y_{T_i})}{P(x_{S_i},y_{S_i})}=\\frac{P(x_{S_i})}{P(x_{T_i})}$$\n",
    "\n",
    "因此只要能估计出等式右侧内容，就可以求解transductive transfer learning问题。这就有很多方法了，以后遇到了再看，比如kernel-mean matching (KMM) algorithm。\n",
    "\n",
    "#### Transferring Knowledge of Feature Representations\n",
    "\n",
    "大部分transductive transfer learning下的Transferring Knowledge of Feature Representations都集中在非监督学习上。这里暂时不表。\n",
    "\n",
    "### UNSUPERVISED TRANSFER LEARNING\n",
    "\n",
    "暂时不看\n",
    "\n",
    "### TRANSFER BOUNDS AND NEGATIVE TRANSFER\n",
    "\n",
    "还有一个重要的内容就是了解迁移学习的不适用情况。Negative transfer 还没有很多的研究，只是一般认为，如果里两任务差别太大，那么就会有这种情况。\n",
    "\n",
    "最后，从另一个角度总结下，给定source和target的$\\mathcal{D}$ （domain） 和 $\\mathcal{T}$ （task），根据他们的不相等程度有了四个迁移学习的场景。对文档分类例子下可以举例如下：\n",
    "\n",
    "1. $\\mathcal{X}_S \\neq \\mathcal{X}_T$。源和目标域的特征空间不同，比如文档是由两个不同语言写的。在NLP中，这是cross-lingual adaption。\n",
    "2. $P(\\mathcal{X}_S) \\neq P(\\mathcal{X}_T)$。源和目标域的边缘概率分布不同，比如文档讨论不同的主题，这称为domain adaption\n",
    "3. $\\mathcal{y}_S \\neq \\mathcal{y}_T$。两个任务的标签空间是不同的。比如目标任务中，文档需要分配不同的标签。这种场景通常和第四个一起出现，极少有标签空间不同的两个不同任务却有相同条件概率分布的。\n",
    "4. $P(Y_S|X_S) \\neq P(Y_T|X_T)$。源和目标任务的条件概率分布不同。比如源文档和目标文档的类别是不平衡的。这种情况在实践和方法中很常见，如过采样、欠采样\n",
    "\n",
    "## 如何使用迁移学习\n",
    "\n",
    "首先，一般来说，在模型开发和评估之前，在领域中使用迁移学习是否有好处并不明显。\n",
    "\n",
    "不过使用迁移学习有几种可能的好处：\n",
    "\n",
    "- Higher start. 源模型上的初始skill（训练成果）(在refine模型之前)比其他情况下更高。\n",
    "- Higher slope. 在源模型的训练过程中，skill改进的速度比不使用源模型时要快。\n",
    "- Higher asymptote. 经过训练的模型的聚合skill的表现比其他方法更好。\n",
    "\n",
    "更形象地，如下图所示：\n",
    "\n",
    "![](Three-ways-in-which-transfer-might-improve-learning-1024x505.png)\n",
    "\n",
    "试试如果你能识别一个有丰富数据的相关的任务并且你有足够的资源来开发一个模型,并能重用这个模型到自己的问题,或者有一个pre-trained模型可用,那么可以使用它作为自己的一个模型起始点。\n",
    "\n",
    "在一些你可能没有太多数据的问题上，迁移学习可以使你开发出熟练的模型，如果没有迁移学习，你根本无法开发出这些模型。\n",
    "\n",
    "前面的类型介绍中已经提到了how to transfer，但是因为比较学术，针对具体算法介绍了，暂时没有展开。这里根据其他blog的查阅，从逻辑上来说主要有两种方式：\n",
    "\n",
    "- Develop Model Approach\n",
    "    1. Select Source Task. 选择一个相关的有大量数据的模型预测问题，在输入数据、输出数据和/或从输入到输出数据的映射过程中学习的概念之间存在某种关系。\n",
    "    2. Develop Source Model. 对选择的第一个任务建立模型。 模型必须比简单模型更好，以确保执行了一些特性学习。\n",
    "    3. Reuse Model. 在源任务上拟合的模型作为第二个任务的模型起始点。这种方式会使用源模型的全部或部分，这取决于所使用的模型技术。\n",
    "    4. Tune Model. 针对目标任务，模型需要针对输入输出数据对作调整和精炼。\n",
    "- Pre-trained Model Approach\n",
    "    1. Select Source Model. 选择一个预训练的模型，有很多研究机构发布了可用的基于大量数据训练的模型可供选择。\n",
    "    2. Reuse Model. 预训练的模型可以用来作为目标模型的起始点。同样的，可使用源模型的全部或部分，这取决于所使用的模型技术。\n",
    "    3. Tune Model. 同样地，针对目标任务，模型需要针对输入输出数据对作调整和精炼。\n",
    "    \n",
    "在深度学习领域，第二种方式更常见。\n",
    "\n",
    "在解决提及的四种迁移学习场景上，迁移学习拥有悠久的研究和技术的历史，前面简单记录的那篇综述文献可以看到一些早期方法的综述。而深度学习的出现导致了一系列迁移学习的新方法，这里就根据blog简单记录一些现在的针对深度学习的一些方法案例。\n",
    "\n",
    "### 使用预训练的 CNN 特征\n",
    "\n",
    "首先针对神经网络补充一点点知识。\n",
    "\n",
    "神经网络需要用数据来训练，在训练的时候我们希望网络能够在多次正向反向迭代的过程中，找到合适的权重，即它从数据中获得信息，进而把它们转换成相应的权重。这些**权重能够被提取出来，迁移到其他的神经网络中**，我们“迁移”了这些学来的特征，就不需要从零开始训练一个神经网络了，也就是说通过使用之前在大数据集上经过训练的预训练模型，我们可以直接使用相应的结构和权重，将它们应用到我们正在面对的问题上。\n",
    "\n",
    "预训练模型(pre-trained model)就是前人为了解决类似问题所创造出来的模型。你在解决问题的时候，不用从零开始训练一个新模型，可以从在类似问题中训练过的模型入手。一个预训练模型可能对于你的应用中并不是100%的准确对口，但是它可以为你节省大量功夫。\n",
    "\n",
    "在选择预训练模型的时候你需要非常仔细，如果你的问题与预训练模型训练情景下有很大的出入，那么模型所得到的预测结果将会非常不准确。**Transfer learning only works in deep learning if the model features learned from the first task are general**.\n",
    "\n",
    "如果预训练模型已经训练得很好，我们就不会在短时间内去修改过多的权重，在迁移学习中用到它的时候，往往只是进行微调(fine tune)。在修改模型的过程中，我们通过会采用比一般训练模型**更低的学习速率**。关于微调整部分可以参考本项目文件夹下的[cnn-mnist.ipynb](https://github.com/OuyangWenyu/elks/blob/master/math-basics/nn/cnn/cnn_mnist.ipynb)。\n",
    "\n",
    "为了了解目前应用中最常见的迁移学习方式，我们必须理解在 ImageNet 上大型卷积神经网络所取得的杰出成功。\n",
    "\n",
    "虽然这些模型工作方式的许多细节仍然是一个谜，但我们现在已经意识到，**较低的卷积层捕获低级图像特征**，例如边缘，而**较高的卷积层捕获越来越复杂的细节**，例如身体部位、面部和其他组合性特征。\n",
    "\n",
    "最后的全连接层通常被认为是**捕获了与解决相应任务相关的信息**，例如 AlexNet 的全连接层可以指出哪些特征与在 1000 个物体类中分类一张图片相关的。\n",
    "\n",
    "然而，尽管知道一只猫有胡须、爪子、毛皮等是将一个动物识别为猫所必需的,但它并不能帮助我们识别新物体或解决其他共同的视觉任务（如场景识别、细粒度识别、属性检测和图像检索）。\n",
    "\n",
    "然而，能够帮助我们的是捕获关于一幅图像是如何组成的以及它包括什么样的边缘和形状组合的通用信息。正如我们之前所描述的，这种信息被包含在在 ImageNet 上训练出的**卷积神经网络的最后卷积层或者最后一层之前的全连接层**。\n",
    "\n",
    "所以，对于一个新任务，我们可以简单地使用这种由最先进的卷积神经网络在 ImageNet 上预训练出的现成特征，并在这些抽象的特征上面训练出新的模型。在实际中，我们要么**保持预训练的参数固定不变，要么以一个较小的学习率来调节它们**，以保证我们不会忘记之前学习到的知识。\n",
    "\n",
    "![](1_oCZ4n4iaOYTbgsMkSEJSgA.png)\n",
    "\n",
    "### 学习图像的隐含结构\n",
    "\n",
    "一个类似的假设被用来推动生成模型：当训练一个生成模型的时候，我们**假设生成逼真图像的能力需要对图像隐含结构的理解**，它反过来可以被用在很多其他任务中。这个假设本身也依赖一个前提，即**所有图像都由一个低维度的流形结构决定**，这就是说，可以用一个模型来抽象出图像的隐含结构。利用生成对抗网络来生成逼真图片的最新进展揭示，这种模型或许真的存在。\n",
    "\n",
    "预训练的特征在视觉之外的领域有用吗？\n",
    "\n",
    "现成的卷积神经网络特征在视觉任务上有着无出其右的结果，但是问题在于，**使用其它的数据，在其它领域（例如语言）能否复制这种成功？** 目前，对自然语言处理而言没有能够实现与图像领域一样令人惊叹结果的现成特征。为什么呢？这样的特征是否存在呢？如果不存在，那为什么视觉比语言更有利于这种形式的迁移呢？\n",
    "\n",
    "诸如词性标注或分块这类低级别任务的输出就可以被比作现成的特征，但是在没有语法学的帮助下，不能采集到更细粒度的特征。正如我们看到的，可概括的现成特征似乎存在似乎存在于众多任务中原型任务中。在视觉中，物体识别就占据了这样的地位。在语言中，最接近的类似物可能是语言建模：为了预测给定词汇序列中的下一个单词或者句子，模型需要处理语言是如何构造的知识，需要理解那些单词很可能是相关的，那些单词很可能是彼此跟随出现的，需要对长期依赖关系进行建模，等等。\n",
    "\n",
    "使用预训练特征是目前做迁移学习的最直接、最常用的方式。不过，到目前为止它并不是唯一的一种。\n",
    "\n",
    "### 学习域不变的表征\n",
    "\n",
    "在实际中，预训练特征通常被用在我们想适应的新任务的场景3中。 对其他场景而言，另一个由深度学习实现知识迁移的方式是学习**基于域而不会改变的表征**。给非视觉任务创建基于特定域的不变表征要比为所有任务生成有用的表征要更加经济，更加可行。\n",
    "\n",
    "### 让表征更加相似\n",
    "\n",
    "为了提高学到的表征从源域到目标域的可迁移性，我们希望两个域之间的表征尽可能相似，这样一来，我们就不用考虑可能阻碍迁移的特定域的特征，只需要考虑域之间的共同点。\n",
    "\n",
    "与其仅仅让我们的自动解码器学到一些表征，不如积极地激励以让两个域中的表征和彼此变得更加相似。我们可以像预处理步骤一样把这个直接应用在我们的数据表证过程中，然后把新的表征用来训练。我们也可以促使我们的模型中的表征变得更加相似\n",
    "\n",
    "### 混淆域\n",
    "\n",
    "另一个最近变得流行的、用来确保两个域的表征之间相似性的方式就是在现有的模型上增加一个目标函数来鼓励两个域的混淆。这个域混淆的损失函数就是常规的分类损失函数，模型尝试预测输入样例的类别。然而，又和常规的损失函数有所不同，从损失函数到网络的剩余部分的流动是反向的。\n",
    "\n",
    "### Learning from simulations\n",
    "\n",
    "一个非常在将来会见到更多的迁移学习应用就是**从模拟中学习**。对很多依靠硬件来交互的机器学习应用而言，在现实世界中收集数据、训练模型，要么很昂贵，要么很耗时间。所以最好能以某些风险较小的其他方式来收集数据。\n",
    "\n",
    "模拟是针对这个问题的首选工具，在现实世界中它被用来实现很多先进的机器学习系统。从模拟中学习并将学到的知识应用在现实世界，这是迁移学习场景 2 中的实例，因为源域和目标域的特征空间是一样的（仅仅依靠像素），但是模拟和现实世界的边际概率分布是不一样的，即模拟和目标域中的物体看上去是不同的，尽管随着模拟的逐渐逼真，这种差距会消失。同时，模拟和现实世界的条件概率分布可能是不一样的，因为模拟不会完全复制现实世界中的所有反应，例如，一个物理引擎不会完全模仿现实世界中物体的交互。\n",
    "\n",
    "从模拟中学习有利于让数据收集变得更加容易，因为物体可以容易地被限制和分析，同时实现快速训练，因为学习可以在多个实例之间并行进行。因此，这是需要与现实世界进行交互的大规模机器学习项目的先决条件，例如自动驾驶汽车。谷歌无人车的技术主管 Zhaoyin Jia 说，「如果你真的想做无人驾驶车，模拟是必需的」。\n",
    "\n",
    "另一个方向是通向通用人工智能的途径，其中模拟会是一个必需的组成部分。**在现实世界中直接训练一个代理来实现通用人工智能的代价太高**，并且不必要的复杂度还会在初始的时候阻碍训练。相反，如果基于诸如CommAI-env的模拟环境的话，学习也许会更加成功.\n",
    "\n",
    "### Adapting to new domains\n",
    "\n",
    "尽管从模拟中学习是域适应的一个特例，总结一些域适应的其他例子也是有价值的。\n",
    "\n",
    "比如域适应在视觉中是一个常规的需求，因为标签信息易于获取的数据和我们实际关心的数据经常是不一样的，无论这涉及识别自行车还是自然界中的其他物体。即使训练数据和测试数据看起来是一样的，训练数据也仍然可能包含人类难以察觉的偏差，而模型能够利用这种偏差在训练数据上实现过拟合。\n",
    "\n",
    "## 相关的其他研究\n",
    "\n",
    "迁移学习并不是唯一一个试图利用有限的数据、在新的任务上使用学到的知识、并让模型在新环境上具有较好的泛化能力的机器学习领域。所以，在下面的内容中，我们会介绍一些其他的与迁移学习相关或者能够补充迁移学习目标的方向。\n",
    "\n",
    "### 半监督学习\n",
    "\n",
    "迁移学习力图最大效率地使用某些任务或者域中的无标签数据。这也是半监督学习所恪守的准则，半监督学习遵循经典机器学习的设定，但是它仅仅采用有限数量的标签数据来训练。如此，半监督域适应本质上就是在域变化的情况下进行半监督学习。许多来自于半监督学习的教训和思想同样地适用于迁移学习。文献 [Semi-Supervised Learning Literature Survey]() 是一个很不错的关于半监督学习的综述。\n",
    "\n",
    "### 更有效地使用可用的数据\n",
    "\n",
    "另外一个与迁移学习和半监督学习相关的方向是让模型在有限数量的数据上运行得更好。\n",
    "\n",
    "这个可以用几种方式实现：你可以使用无监督学习或者半监督学习从无标签数据中抽取信息，以减少对有标签样本的依赖；你可以允许模型能够获取一些数据的固有属性，同时减轻正则化过程中的过拟合倾向；最后，你还可以使用至今仍然被忽视或者位于不明显的地方的一些数据。\n",
    "\n",
    "作为用户生成内容的意外结果，这种巧合的数据可能会被创建，例如能够提升命名实体和词性标注的超链接；它也可能作为注释的副产品存在，例如注释器不一致（annotator disagreement) 可能改进标注或者解析；或者来源于用户行为的数据，例如视线追踪或者键入变化，可以影响自然语言处理的任务。虽然这些数据只是以有限的方式被利用，但是这样的例子鼓励我们在意外的地方查找数据，并研究检索数据的新方法。\n",
    "\n",
    "### 提高模型的泛化能力\n",
    "\n",
    "让模型更好地泛化也是与此相关的一个方向。为了实现更好的泛化能力，我们首先必须理解大规模神经网络的行为和错综复杂的结构，并且去研究它们泛化的原因和方式。最近的工作已经朝着这个目标迈出了大有希望的步伐，但是很多问题仍然等待解答。\n",
    "\n",
    "### 让模型更加稳健（robust)\n",
    "\n",
    "尽管提升我们的模型的泛化能力这方面已经比较成功了，在类似的例子上面我们也许泛化得很好，但是在出乎意料或者者非典型的输入情况下仍然会失败。所以，一个关键的补充目标就是让我们的模型更加稳健。在近来对抗学习的进步的推动下，这个方向越来越受关注，并且，最近的方法研究了很多让模型在最糟糕的情况下或者面对不同设置的对抗样本时变得更加稳健的方式\n",
    "\n",
    "### 多任务学习\n",
    "\n",
    "在迁移学习中，我们主要关心在我们的目标任务和域上友好的表现。相反，多任务学习中的目标是在所有可用的任务上都要表现良好，尽管某个标签数据通常都被假定在一个任务上。虽然多任务学习的方法没有直接用在迁移学习上，但是对多任务学习有利的关于任务的思想仍然能够指引迁移学习的决策。\n",
    "\n",
    "单任务学习VS多任务学习\n",
    "\n",
    "单任务学习：一次只学习一个任务（task），大部分的机器学习任务都属于单任务学习。\n",
    "\n",
    "多任务学习：把多个相关（related）的任务放在一起学习，同时学习多个任务。\n",
    "\n",
    "多任务学习（multitask learning）产生的原因？\n",
    "\n",
    "大多数机器学习任务都是单任务学习。复杂的问题，也可以分解为简单且相互独立的子问题来单独解决，然后再合并结果，得到最初复杂问题的结果。不过把现实问题当做一个个独立的单任务处理，忽略了问题之间所富含的丰富的关联信息，那么多任务就有效？\n",
    "\n",
    "多个任务之间共享一些因素，它们可以在学习过程中，共享它们所学到的信息，这是单任务学习所具备的。相关联的多任务学习比单任务学习能去的更好的泛化（generalization）效果。\n",
    "\n",
    "多任务学习时，多个任务之间的模型空间（Trained Model）是共享的。如下图：\n",
    "\n",
    "![](v2-9eed3a14f160f9562a37eafe82991b8e_hd.png)\n",
    "\n",
    "假设用含一个隐含层的神经网络来表示学习一个任务，单任务学习和多任务学习可以表示成如下图所示。\n",
    "\n",
    "![](v2-2e2316ef5678c50b3b737335f5a0d7e9_hd.png)\n",
    "\n",
    "多任务学习时，多个任务之间的浅层表示共享（shared representation）。\n",
    "\n",
    "### 持续学习\n",
    "\n",
    "虽然多任务学习允许我们在许多任务中保留知识，而不会对我们的源任务造成性能损失，但只有在所有任务都处于训练时间的情况下，这才是可能的。对于每个新任务，我们通常需要重新训练我们所有任务的模型。\n",
    "\n",
    "然而，在现实世界中，我们希望一个代理能够通过使用它以往的一些经验来处理逐渐变得复杂的任务。为了达到这个目的，我们需要让一个模型在不忘记的情况下持续地学习。这个机器学习的领域被称为学会学习、元学习、终生学习，或者持续学习。持续学习在最近的强化学习 (强化学习以 Google DeepMind 对通用学习代理的探索而著称) 上已经取得了成功，也正在被用于序列到序列的模型上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
