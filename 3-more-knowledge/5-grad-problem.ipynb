{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c754d85e-9d7c-46d2-9c66-c29a25e8bc1d",
   "metadata": {},
   "source": [
    "## 梯度的问题\n",
    "\n",
    "有时候在计算中，我们如果自定义了一些函数，很有可能会出现梯度崩掉的情况，通常是梯度爆炸（梯度NaN）或者梯度消失（0）\n",
    "\n",
    "下面先看梯度爆炸的例子，主要参考了：https://zhuanlan.zhihu.com/p/79046709\n",
    "\n",
    "其他资料：\n",
    "\n",
    "- https://blog.csdn.net/mch2869253130/article/details/111034068\n",
    "\n",
    "它是一个loss函数中有幂函数的情况，即 $\\Gamma(x_i) = x ^ r$，其中r是0到1之间的数\n",
    "\n",
    "这个loss函数在反向传播过程中很可能会遇到梯度爆炸，因为反向传播的过程是对loss链式求一阶导数的过程：\n",
    "\n",
    "$$\\frac{d\\Gamma(x_i)}{dx_i}=\\frac{r}{{x_i}^{1-r}}$$\n",
    "\n",
    "出现了 1/x 的情况，这就会出现梯度崩掉的情况，为了避免这种情况，手动设置下条件，让$\\Gamma(x_i)$变成个条件判断的函数，它是这样定义的，就是 x<0.003时候，定义成12.9 * x，其余时候还是原函数，按理说，就不会有崩掉的情况了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c15cd8e-67f2-4bd5-8adb-d8abec837be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda0b4bd-32fd-40dd-a896-bf564886d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "loss = mse(X, gamma_inv(X))\n",
    "\"\"\"\n",
    "def loss_function(x):\n",
    "    mask = (x < 0.003).float()\n",
    "    print(\"mask:\", mask)\n",
    "    gamma_x = mask * 12.9 * x + (1-mask) * (x ** 0.5)\n",
    "    loss = torch.mean((x - gamma_x) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a90fb5-9690-4c7e-8357-e49e25cb952d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: tensor([1., 1., 0., 0., 0.])\n",
      "loss: tensor(0.0105, grad_fn=<MeanBackward0>)\n",
      "tensor([    nan,  0.1416, -0.0243, -0.0167,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0, 0.0025, 0.5, 0.8, 1], requires_grad = True)\n",
    "loss = loss_function(x)\n",
    "print('loss:', loss)\n",
    "loss.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf45aa1-00a1-44d2-9fb0-d4757df9a733",
   "metadata": {},
   "source": [
    "改进后的公式是一个分支结构，在实现时，就采用了类似于Matlab中矩阵计算的mask方式，满足条件的$x_i$在mask中对应位置的值为1，因此， 这个公式的结构只会保留 x<0.003 的结果，同样的道理， 1-mask 就保留另一部分，合一块就实现了上述改进后的公式。但是从实现的情况看，显然不是这样的，nan还是有，结果还是崩了。\n",
    "\n",
    "换成where语句了也是一样的，还是会有这个问题，可能是where的具体实现就是上面这样mask来实现的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77544415-3f43-4fe8-a8aa-16ad07fc6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function1(x):\n",
    "    gamma_x = torch.where(x < 0.003, 12.9 * x, x ** 0.5)\n",
    "    loss = torch.mean((x - gamma_x) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4585780-436d-46d8-bd8a-dbe76e13a1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.0105, grad_fn=<MeanBackward0>)\n",
      "tensor([    nan,  0.1416, -0.0243, -0.0167,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor([0, 0.0025, 0.5, 0.8, 1], requires_grad = True)\n",
    "loss1 = loss_function1(x1)\n",
    "print('loss:', loss1)\n",
    "loss1.backward()\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010faeda-60b6-43e6-b70a-bde7c9180160",
   "metadata": {},
   "source": [
    "上面的过程在Python解释器中解释或许是这样的：\n",
    "\n",
    "1. 计算 mask * 12.9的时候是对mask进行广播式的乘法，结果为：原本为1的位置变为了12.9，原本为0的位置依旧为0；\n",
    "2. 将1.的结果继续与x相乘，本质上仍然是与x的每个元素相乘，只是mask中不满足条件的 $x_i$ 位置为0，表现出的结果是仅对满足条件的 $x_i$  进行了计算；\n",
    "3. 同理，$\\Gamma(x_i)$公式的后半部分也是同样的计算过程，即，x  中的每个值依旧会进行 $x^{0.5}$ 的计算；\n",
    "\n",
    "按照上述过程进行前向传播，在反向传播时，梯度不是从某一个分支得到的，而是两个分支相加得到的，换句话说，依旧没能解决梯度变为nan的问题。\n",
    "\n",
    "所以问题是 $x_i$=0 依旧参与了幂次运算，导致在反向传播时计算出的梯度为nan。\n",
    "\n",
    "要解决这个问题，就要保证在 $x_i=0$ 时不会进行这样的计算。\n",
    "\n",
    "新的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d567de79-919a-4911-a7d9-208c2cec17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function2(x):\n",
    "    mask = x < 0.003\n",
    "    print(\"mask:\", mask)\n",
    "    gamma_x = torch.zeros(x.size())\n",
    "    gamma_x[mask] = 12.9 * x[mask]\n",
    "    mask = x >= 0.003\n",
    "    gamma_x[mask] = x[mask] ** 0.5\n",
    "    loss = torch.mean((x - gamma_x) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd14623-1396-4511-91a9-05501e2497fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: tensor([ True,  True, False, False, False])\n",
      "loss: tensor(0.0105, grad_fn=<MeanBackward0>)\n",
      "tensor([ 0.0000,  0.1416, -0.0243, -0.0167,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.tensor([0, 0.0025, 0.5, 0.8, 1], requires_grad = True)\n",
    "loss2 = loss_function2(x2)\n",
    "print('loss:', loss2)\n",
    "loss2.backward()\n",
    "print(x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7da78a-d0ee-464c-972f-bc776d2273e9",
   "metadata": {},
   "source": [
    "可以看到，这时候梯度不再为nan了。因为这里改变了对于 $\\Gamma(x_i)$ 分支的处理方式，先是构建了mask，但是随后不是一次性让gamma_x得到计算，而是分开，小于的单独计算一波，大于的再计算一波。这样等于0的那部分就没参与到幂次运算中，这样求导的时候不会有问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6344c-ef92-40b2-aef1-7d23a2debf52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
