{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "younger-mention",
   "metadata": {},
   "source": [
    "# 权重初始化\n",
    "\n",
    "本文主要参考了以下资料以简单了解神经网络权重初始化的相关基本概念。\n",
    "\n",
    "- [Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)\n",
    "- [Initializing neural networks](https://www.deeplearning.ai/ai-notes/initialization/#IV)\n",
    "\n",
    "## 为什么要初始化权重\n",
    "\n",
    "初始化权重是神经网络训练的第一步，合适的权重初始化能防止深层神经网络的前向过程中输出的梯度爆炸/消失。如果出现梯度爆炸/消失，loss的梯度将太大或太小而无法有益地反向传播，并且即使可以做到，NN也将花费更长的时间收敛。\n",
    "\n",
    "比如将所有权重初始化为0会导致神经元学习到相同的特征。实际上，任何常量式的初始化方式都会表现地很差。考虑一个仅有两个隐含单元的神经网络，假设我们初始化所有biases为0，权重为常数$\\alpha$。如果我们前向输入 ($x_1,x_2$)，每个输出都将是 $relu(\\alpha x_1 + \\alpha x_2)$。每个隐含单元会对cost有相同的影响，这会导致一样的梯度，那么自然神经元值就会以相同的方式变化，这样神经元就很难学习到不同的规律了。\n",
    "\n",
    "即使初始化权重是变化的，也要注意当权重值初始化过大时会导致发散，过小时会导致学习过慢。下面看一看梯度爆炸和消失的问题。\n",
    "\n",
    "考虑一个9层的神经网络。\n",
    "\n",
    "![](img/9layer.png)\n",
    "\n",
    "假设所有激活函数是线性的，输出有：\n",
    "\n",
    "$\\hat y = a^{[L]}=W^{[L]}W^{[L-1]}W^{[L-2]}\\cdots W^{[3]}W^{[2]}W^{[1]}x$\n",
    "\n",
    "其中，L=10，$W^{[1]}, W^{[2]}, \\cdots W^{[L-1]}$都是2\\*2矩阵。假设$W^{[1]}=W^{[2]}= \\cdots =W^{[L-1]}=W$，那么有 $\\hat y=W^{[L]}W^{L-1}x$，其中$W^{L-1}$表示L-1次方。\n",
    "\n",
    "那么可以想到，当权值过大时，很容易在幂次后变得很大，即cost相对于参数的梯度非常大，这会使cost在最小值周围震荡；而当太小时，则梯度会很快接近0，使得cost还没到最小就不怎么变化了。\n",
    "\n",
    "下面是一些例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opening-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adolescent-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准正态分布\n",
    "x = torch.randn(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-invention",
   "metadata": {},
   "source": [
    "假设有100层的神经网络，没有激活函数。每层一个矩阵a包括层的权重。那么就是100个连续矩阵的乘法来执行前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-consultation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan), tensor(nan))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(100): \n",
    "    a = torch.randn(512,512)\n",
    "    x = a @ x # @是pytorch中矩阵相乘的运算\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-morris",
   "metadata": {},
   "source": [
    "可以看到输出太大以至于计算机识别不了均值和标准差了。可以进一步看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "center-charity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):\n",
    "    a = torch.randn(512,512)\n",
    "    x = a @ x\n",
    "    if torch.isnan(x.std()): break\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-thread",
   "metadata": {},
   "source": [
    "可以看到28层就已经梯度爆炸了。\n",
    "\n",
    "也就是说将权重初始化范围定到和输入数据相同的分布里并不是一个好方法，容易导致梯度爆炸。\n",
    "\n",
    "梯度消失的情况如下，将标准差缩小到0.01："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hollywood-williams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100): \n",
    "    a = torch.randn(512,512) * 0.01\n",
    "    x = a @ x\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-format",
   "metadata": {},
   "source": [
    "那么如何选取合适的初始化值呢\n",
    "\n",
    "## 如何找到合适的初始化值\n",
    "\n",
    "在前向计算中，假设仅考虑线性变换，则输出y就是输入x和权重a的乘积$y_i=\\sum _{k=0}^{n-1}a_{i,k}x_k$，其中i就是权重a的行序号，k是列序号。\n",
    "\n",
    "可以证明在给定的层，输入x和从标准正态分布初始化的权重矩阵a的矩阵乘积，平均而言，有一个非常接近于输入连接数的平方根的标准差，在上面的实例中就是 $\\sqrt{512}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "knowing-dover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00028589955177158117, 22.625784281824924)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512,512)\n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abstract-template",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.627416997969522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-oregon",
   "metadata": {},
   "source": [
    "从定义矩阵乘法的角度看待该属性，这个特点并不意外：为了计算y，我们相加了512个 输入x的一个元素与权重a的一列的element-wise 乘积 。在我们使用标准正态分布初始化x和a的示例中，这512个乘积中的每一个乘积均值为0，标准偏差为1。所以512个乘积加起来有均值0和方差512，所以有标准差$\\sqrt{512}$\n",
    "\n",
    "如果我们首次归一化就选择除以$\\sqrt{512}$，那么y的输出就会有$1/\\sqrt{512}$的方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bulgarian-germany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0007510259594317177, 1.0156330218685454)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)\n",
    "    y = a*x\n",
    "    mean += y.item()\n",
    "    var += y.pow(2).item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demanding-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005906981339892183, 0.0019873223970937547)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)*math.sqrt(1./512)\n",
    "    y = a*x\n",
    "    mean += y.item()\n",
    "    var += y.pow(2).item()\n",
    "mean/10000, var/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "blank-charleston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001953125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-bottle",
   "metadata": {},
   "source": [
    "这意味着矩阵y的标准偏差（包含通过输入x和权重a之间的矩阵乘法生成的 512 个值中的每一个值）将为 1。通过实验进行确认："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "revised-municipality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001038469743024325, 1.0000861601606938)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512,512)*math.sqrt(1./512)\n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-threat",
   "metadata": {},
   "source": [
    "现在，让我们重新运行我们的100层网络。与以前一样，我们首先选择层的权重从标准正态分布内[-1,1]随机的，但这次我们通过1 /√n 缩放权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "applicable-regard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0142), tensor(0.7058))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):\n",
    "    a = torch.randn(512,512) * math.sqrt(1./512)\n",
    "    x = a @ x\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-madagascar",
   "metadata": {},
   "source": [
    "可以看到在100个假设的层之后，不会有爆炸或消失。\n",
    "\n",
    "## Xavier初始化\n",
    "\n",
    "现在考虑实际神经网络的初始化。\n",
    "\n",
    "为了防止梯度消失和爆炸，通常我们会遵守如下的规律：\n",
    "\n",
    "1. 激活值的均值为0\n",
    "2. 激活值的方差每层保持不变\n",
    "\n",
    "在这样两个假设下，反向传播梯度不会变的太大或太小。\n",
    "\n",
    "更具体地，考虑一个层l。其前向传播是：\n",
    "\n",
    "$$a^{[l-1]}=g^{[l-1]}(z^{[l-1]})$$\n",
    "$$z^{[l]}=W^{[l]}(a^{[l-1]})+b^{[l]}$$\n",
    "$$a^{[l]}=g^{[l]}(z^{[l]})$$\n",
    "\n",
    "我们希望有：\n",
    "\n",
    "$$E[a^{[l-1]}]=E[a^{[l]}]$$\n",
    "$$Var[a^{[l-1]}]=Var[a^{[l]}]$$\n",
    "\n",
    "确保零均值并保持每一层输入的方差值，可以保证不会梯度爆炸/消失，我们稍后将对此进行解释。\n",
    "\n",
    "此方法既适用于正向传播(用于activations，即激活函数的输出值)，也适用于反向传播(用于cost相对于activations的梯度)。对于每一层l，推荐的初始化是Xavier初始化(或其派生方法之一)：\n",
    "\n",
    "$$W^{[l]}\\sim \\mathscr N(\\mu =0,\\sigma ^2=\\frac{1}{n^{[l-1]}})$$\n",
    "$$b^{[l]}=0$$\n",
    "\n",
    "也就是说，l层的所有权值都随机地从均值为$\\mu =0$，方差为$\\sigma^2 = \\frac{1}{n^{[l-1]}}$ 的正态分布中选取，其中$n^{[l-1]}$是l-1层神经元的数目。偏差用零初始化。\n",
    "\n",
    "下面是例子，激活函数使用tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pressing-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x): \n",
    "    return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unexpected-outreach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0028), tensor(0.0680))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100): \n",
    "    a = torch.randn(512,512) * math.sqrt(1./512)\n",
    "    x = tanh(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-proceeding",
   "metadata": {},
   "source": [
    "可以看到100层之后，激活函数输出的标准差约为0.06，这有些小了，不过激活值没有消失。\n",
    "\n",
    "此方法直到Xavier等人的2010年文章都不是“标准”方法，此前人们还是用的-1到1均匀分布后用$1/\\sqrt n$scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "steady-durham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.2154e-26), tensor(1.0737e-24))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100): \n",
    "    a = torch.Tensor(512,512).uniform_(-1, 1) * math.sqrt(1./512)\n",
    "    x = tanh(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-discretion",
   "metadata": {},
   "source": [
    "可以看到这种方法基本上activations就消失了。\n",
    "\n",
    "从数学论证角度简单了解Xavier Initialization 可以参考[这里](https://www.deeplearning.ai/ai-notes/initialization/#IV)\n",
    "\n",
    "实践中，机器学习工程师也会用$\\mathscr N(\\mu =0,\\sigma ^2=\\frac{1}{n^{[l-1]}})$或者$\\mathscr N(\\mu =0,\\sigma ^2=\\frac{2}{n^{[l-1]+n^{[l]}}})$来初始化作为Xavier 初始化。\n",
    "\n",
    "根据[这个blog](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)的介绍，Xavier论文里用的是$\\pm\\frac{\\sqrt 6}{\\sqrt{n_i+n_{i+1}}}$，其中$n_i$是入层fan-in的神经元个数，$n_{i+1}$是fan-out的神经元个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "horizontal-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier(m,h): \n",
    "    return torch.Tensor(m, h).uniform_(-1, 1)*math.sqrt(6./(m+h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "honest-crown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0023), tensor(0.0657))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):\n",
    "    a = xavier(512, 512)\n",
    "    x = tanh(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-cooper",
   "metadata": {},
   "source": [
    "## Kaiming 初始化\n",
    "\n",
    "对于非对称的激活函数，比如ReLU，更常用的是Kaiming初始化方法。下面简单看看例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "extreme-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "congressional-arabic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.022719645738603, 16.00082034854704)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512,512)\n",
    "    y = relu(a @ x)\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-workshop",
   "metadata": {},
   "source": [
    "结果是，当使用reu激活时，单层的平均标准差将非常接近输入连接数的平方根，除以2的平方根，在我们的例子中是√512/√2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pursuant-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(512/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-yukon",
   "metadata": {},
   "source": [
    "将权重矩阵a的值按这个数字缩放将使每个ReLU层的平均标准差为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "social-arabic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5644591210246086, 1.000717541461769)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512,512) * math.sqrt(2/512)\n",
    "    y = relu(a @ x)\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-checkout",
   "metadata": {},
   "source": [
    "正如之前所展示的，将层激活的标准偏差保持在1左右将允许我们在深度神经网络中堆叠更多的层，而不会发生梯度爆炸或消失。再和Xavier方法对比下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "intelligent-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming(m,h): \n",
    "    return torch.randn(m,h)*math.sqrt(2./m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "automatic-compression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3214), tensor(0.4873))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100): \n",
    "    a = kaiming(512, 512)\n",
    "    x = relu(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "skilled-wealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.3556e-16), tensor(4.8216e-16))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):\n",
    "    a = xavier(512, 512)\n",
    "    x = relu(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-spirituality",
   "metadata": {},
   "source": [
    "可以看到在ReLU中，Xavier方法是会出现激活值消失的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
