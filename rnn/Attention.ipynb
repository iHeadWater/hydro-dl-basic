{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "近几年来，在序列预测中，神经网络也得到了非常快速地发展，有必要及时跟上了解这些模型并应用。\n",
    "\n",
    "本文主要参考以下文章以了解attention的基本原理和简单使用方法。\n",
    "\n",
    "- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "\n",
    "序列模型比较经典的应用还是机器翻译等领域，在2016年底，谷歌翻译就使用了序列模型。不过要充分理解模型以实现它们，需要阐明一系列建立在彼此之上的概念。这就是上面第一篇blog中介绍的内容，并且以可视化的形式给了出来，以此为基础可以更好地理解这一领域的文献。\n",
    "\n",
    "序列到序列模型 sequence-to-sequence或者seq2seq模型 顾名思义，就是输入序列项，输出也是序列。\n",
    "\n",
    "模型一般包括一个encoder，一个decoder。encoder处理输入序列的每项，并将它们的信息编码到一个向量中，这个vector称之为context。在处理完输入序列所有信息后，encoder将context传递给decoder，从而一项项地输出。\n",
    "\n",
    "Context是一个向量，基本上就是一个数组。encoder和decoder通常都是循环神经网络RNN。context的size可设置，通常是encoder RNN的hidden units的数目，一般是256，512，1024等。\n",
    "\n",
    "通过设计，RNN每步能吸收两个输入，一个输入和一个hidden state。在机器翻译中，为了将语言转换为向量，使用了一种称为 word embedding的算法，这一算法能将词转换到向量空间，空间里是能捕获很多语义信息的，比如 king - man + woman = queen.\n",
    "\n",
    "输入进入RNN后，产生两个输出，如下图所示，一个是用于下一步的hidden state，一个是本步输出。\n",
    "\n",
    "![](pictures/QQ截图20210312153849.png)\n",
    "\n",
    "接着RNN会利用第二个输入向量和刚刚输出的hidden state来生成一个新的hidden state和第二个输出。依次循环下去。最后一个hidden state就是context，接着会把它传入decoder，如下所示，一个三向量的输入，最后一个hidden state 即第三号，就会给到decoder\n",
    "\n",
    "![](pictures/QQ截图20210312154719.png)\n",
    "\n",
    "decoder也有一个RNN，会以context为第一输入，自己也有一个hidden state，这样循环下去，每步给一个输出出来。\n",
    "\n",
    "了解完这些之后可以关注下attention。\n",
    "\n",
    "上文中提到的context已经被证明是这类模型的瓶颈所在，使得处理长序列的时候面临挑战。解决这一问题的方式叫做 Attention，极大地提升了机器翻译系统的质量。Attetion允许模型根据需要关注输入序列的相关部分。\n",
    "\n",
    "现在encoder不只是传递最后一个hidden state到decoder，而是传递所有的hidden states到decoder。\n",
    "\n",
    "![](pictures/QQ截图20210312162906.png)\n",
    "\n",
    "第二，为了如前所述实现对当前解码的那一步的相关部分的特别关注，一个attention的decoder在产生输出之前要多做一步：\n",
    "\n",
    "1. 查看decoder接受的所有来自encoder的hidden states，每个 encoder hidden states 都和输入序列中的某个词最相关\n",
    "2. 给每个hidden states打分（暂时先别管怎么算的)\n",
    "3. 将每个 hidden states 乘以它的 softmaxed score, 然后高分的放大hidden states，低分的抹去hidden states\n",
    "\n",
    "![](pictures/QQ截图20210312164338.png)\n",
    "\n",
    "decoder这侧，每一步都完成上面的过程。\n",
    "\n",
    "![](pictures/QQ截图20210312164506.png)\n",
    "\n",
    "可以看到，对于机器翻译来说，每个词都有一个或者几个特别对应的地方，但是对于径流预报，更多地还是有前序的累计效果，类似于卷积一类的操作可能能比attention起到更好的效果，或者softmax的时候不要搞得不同的hidden states之间权重差太多，这需要进一步实践验证，或者能可视化一下效果，类似于他们在机器翻译中做的这样：\n",
    "\n",
    "![](pictures/QQ截图20210312164750.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
